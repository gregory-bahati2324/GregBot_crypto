{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "034efcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dc38b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('merged_shuffled_data_10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77362fbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaler_15m.pkl']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FEATURES = ['RSI', 'EMA12', 'EMA26', 'MACD', 'Signal', 'Histogram', 'DEMA9', 'SMA', 'TSI', '%K', '%D']\n",
    "\n",
    "X = df[FEATURES].values\n",
    "y = df[\"Label\"].values\n",
    "\n",
    "# ===============================\n",
    "# 2. Scale features\n",
    "# ===============================\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "joblib.dump(scaler, \"scaler_15m.pkl\")  # save scaler for later predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb7b3714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 3. Create sliding windows for LSTM\n",
    "# ===============================\n",
    "def create_sequences(X, y, window_size=10):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(window_size, len(X)):\n",
    "        X_seq.append(X[i-window_size:i])  # shape: (window_size, features)\n",
    "        y_seq.append(y[i])\n",
    "    return np.array(X_seq, dtype=np.float32), np.array(y_seq, dtype=np.int64)\n",
    "\n",
    "window_size = 10\n",
    "X_seq, y_seq = create_sequences(X_scaled, y, window_size)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Convert to tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97612909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 4. Compute class weights\n",
    "# ===============================\n",
    "labels, counts = np.unique(y_train.numpy(), return_counts=True)\n",
    "total = sum(counts)\n",
    "class_weights = [total/count for count in counts]  # inverse frequency\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8372def5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 5. Define LSTM Model\n",
    "# ===============================\n",
    "class CryptoLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
    "        super(CryptoLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, (hn, cn) = self.lstm(x)\n",
    "        out = out[:, -1, :]  # last timestep\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "input_dim = len(FEATURES)\n",
    "hidden_dim = 128\n",
    "num_layers = 4\n",
    "output_dim = 3\n",
    "\n",
    "model = CryptoLSTM(input_dim, hidden_dim, num_layers, output_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52fa57da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500, Loss: 1.1034\n",
      "Epoch 2/500, Loss: 1.1030\n",
      "Epoch 3/500, Loss: 1.1050\n",
      "Epoch 4/500, Loss: 1.1029\n",
      "Epoch 5/500, Loss: 1.1025\n",
      "Epoch 6/500, Loss: 1.1026\n",
      "Epoch 7/500, Loss: 1.1029\n",
      "Epoch 8/500, Loss: 1.1029\n",
      "Epoch 9/500, Loss: 1.1028\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     20\u001b[39m batch_X, batch_y = X_train[indices], y_train[indices]\n\u001b[32m     22\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_X\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m loss = criterion(outputs, batch_y)\n\u001b[32m     25\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Trading_bot/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Trading_bot/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mCryptoLSTM.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     out, (hn, cn) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m     out = out[:, -\u001b[32m1\u001b[39m, :]  \u001b[38;5;66;03m# last timestep\u001b[39;00m\n\u001b[32m     13\u001b[39m     out = \u001b[38;5;28mself\u001b[39m.fc(out)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Trading_bot/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Trading_bot/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Trading_bot/venv/lib/python3.12/site-packages/torch/nn/modules/rnn.py:1124\u001b[39m, in \u001b[36mLSTM.forward\u001b[39m\u001b[34m(self, input, hx)\u001b[39m\n\u001b[32m   1121\u001b[39m         hx = \u001b[38;5;28mself\u001b[39m.permute_hidden(hx, sorted_indices)\n\u001b[32m   1123\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1124\u001b[39m     result = \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1125\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1126\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1127\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1128\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1129\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1130\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1131\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1132\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1133\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1134\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1135\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1136\u001b[39m     result = _VF.lstm(\n\u001b[32m   1137\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   1138\u001b[39m         batch_sizes,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1145\u001b[39m         \u001b[38;5;28mself\u001b[39m.bidirectional,\n\u001b[32m   1146\u001b[39m     )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# 6. Loss and optimizer\n",
    "# ===============================\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# ===============================\n",
    "# 7. Training loop\n",
    "# ===============================\n",
    "epochs = 500\n",
    "batch_size = 32\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    permutation = torch.randperm(X_train.size(0))\n",
    "    \n",
    "    for i in range(0, X_train.size(0), batch_size):\n",
    "        indices = permutation[i:i+batch_size]\n",
    "        batch_X, batch_y = X_train[indices], y_train[indices]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/(X_train.size(0)//batch_size):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2c9dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test)\n",
    "    predictions = torch.argmax(outputs, dim=1)\n",
    "\n",
    "accuracy = (predictions == y_test).float().mean()\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906b8422",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"greg_tech_8.pth\")\n",
    "print(\"Model saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1c3a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Example feature values\n",
    "example_features = {\n",
    "    'RSI': 40.231328, \n",
    "    'EMA12': 117510.625832,\n",
    "    'EMA26': 117522.059487,\n",
    "    'MACD': -11.433655,\n",
    "    'Signal':36.277050,\n",
    "    'Histogram':-47.710705,\n",
    "    'DEMA9':117395.915076,\n",
    "    'SMA':117365.470000,\n",
    "    'TSI': 41.065534,\n",
    "    '%K':29.053022,\n",
    "    '%D':42.278097\n",
    "}\n",
    "\n",
    "FEATURES = ['RSI', 'EMA12', 'EMA26', 'MACD', 'Signal', 'Histogram', 'DEMA9', 'SMA', 'TSI', '%K', '%D']\n",
    "\n",
    "# Scale features\n",
    "X_scaled = scaler.transform(df[FEATURES].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8950f3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 10\n",
    "predictions = [1]*window_size  # first window_size rows cannot be predicted, assume \"No Reversal\"\n",
    "\n",
    "for i in range(window_size, len(X_scaled)):\n",
    "    X_input = torch.tensor(X_scaled[i-window_size:i], dtype=torch.float32).unsqueeze(0)  # (1, seq_len, features)\n",
    "    with torch.no_grad():\n",
    "        output = model(X_input)\n",
    "        pred_label = torch.argmax(output, dim=1).item()\n",
    "        predictions.append(pred_label)\n",
    "\n",
    "df[\"Predicted\"] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8fb22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 5. Plot Close prices with predicted reversals\n",
    "# ===============================\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(15,7))\n",
    "plt.plot(df.index, df[\"close\"], label=\"Close Price\", color=\"blue\")\n",
    "\n",
    "# Uptrend reversals (label=2)\n",
    "plt.scatter(df.index[df[\"Predicted\"]==2], df[\"close\"][df[\"Predicted\"]==2],\n",
    "            marker=\"^\", color=\"green\", s=100, label=\"Predicted Uptrend Reversal\")\n",
    "\n",
    "# Downtrend reversals (label=0)\n",
    "plt.scatter(df.index[df[\"Predicted\"]==0], df[\"close\"][df[\"Predicted\"]==0],\n",
    "            marker=\"v\", color=\"red\", s=100, label=\"Predicted Downtrend Reversal\")\n",
    "\n",
    "plt.title(\"Predicted Reversals on Close Prices\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Price\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
