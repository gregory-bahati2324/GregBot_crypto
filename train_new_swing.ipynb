{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d957d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimized_cnn_lstm_triplebarrier_pipeline.py\n",
    "# Run in a Python env with torch + ta + sklearn + pandas + numpy\n",
    "\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, WeightedRandomSampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c204227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# 0) Device\n",
    "# ---------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c97c454e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 1) Feature engineering\n",
    "# ---------------------------\n",
    "import ta  # pip install ta\n",
    "import talib  # optional but used earlier; if missing remove DEMA or use pandas ewm\n",
    "\n",
    "def add_features(df):\n",
    "    df = df.copy()\n",
    "    # ensure columns lower-case/consistent\n",
    "    # expect df to contain 'open','high','low','close','volume' (case-insensitive)\n",
    "    df.columns = [c.lower() for c in df.columns]\n",
    "\n",
    "    # basic indicators (your current set)\n",
    "    df['rsi'] = ta.momentum.RSIIndicator(df['close'], window=14).rsi()\n",
    "    df['ema12'] = df['close'].ewm(span=12, adjust=False).mean()\n",
    "    df['ema26'] = df['close'].ewm(span=26, adjust=False).mean()\n",
    "    df['macd'] = df['ema12'] - df['ema26']\n",
    "    df['signal'] = df['macd'].ewm(span=9, adjust=False).mean()\n",
    "    df['histogram'] = df['macd'] - df['signal']\n",
    "\n",
    "    # DEMA (if talib available)\n",
    "    try:\n",
    "        df['dema9'] = talib.DEMA(df['close'].values, timeperiod=9)\n",
    "    except Exception:\n",
    "        df['dema9'] = df['close'].ewm(span=9).mean()  # fallback\n",
    "\n",
    "    df['sma3'] = ta.trend.sma_indicator(df['close'], window=3)\n",
    "\n",
    "    # TSI\n",
    "    def compute_tsi(close, r1=25, r2=13):\n",
    "        delta = close.diff()\n",
    "        ema1 = delta.ewm(span=r1, adjust=False).mean()\n",
    "        ema2 = ema1.ewm(span=r2, adjust=False).mean()\n",
    "        abs_delta = delta.abs()\n",
    "        abs_ema1 = abs_delta.ewm(span=r1, adjust=False).mean()\n",
    "        abs_ema2 = abs_ema1.ewm(span=r2, adjust=False).mean()\n",
    "        tsi = 100 * (ema2 / (abs_ema2.replace(0, np.nan)))\n",
    "        return tsi.fillna(0)\n",
    "    df['tsi'] = compute_tsi(df['close'])\n",
    "\n",
    "    # Stochastic %K and %D\n",
    "    period = 14; smooth_k = 3; smooth_d = 3\n",
    "    lowest_low = df['low'].rolling(period).min()\n",
    "    highest_high = df['high'].rolling(period).max()\n",
    "    df['%k'] = 100 * (df['close'] - lowest_low) / (highest_high - lowest_low + 1e-8)\n",
    "    df['%k'] = df['%k'].rolling(smooth_k).mean()\n",
    "    df['%d'] = df['%k'].rolling(smooth_d).mean()\n",
    "\n",
    "    # --- NEW HIGH-IMPACT features ---\n",
    "    # ATR (volatility)\n",
    "    df['atr'] = ta.volatility.AverageTrueRange(df['high'], df['low'], df['close'], window=14).average_true_range()\n",
    "    # On-Balance Volume\n",
    "    df['obv'] = ta.volume.OnBalanceVolumeIndicator(df['close'], df['volume']).on_balance_volume()\n",
    "    # ADX (trend strength)\n",
    "    df['adx'] = ta.trend.ADXIndicator(df['high'], df['low'], df['close'], window=14).adx()\n",
    "    # CCI\n",
    "    df['cci'] = ta.trend.cci(df['high'], df['low'], df['close'], window=20)\n",
    "\n",
    "    # Price-derived features\n",
    "    df['return_1'] = df['close'].pct_change()\n",
    "    df['roll_mean_5'] = df['close'].rolling(5).mean()\n",
    "    df['roll_std_5'] = df['close'].rolling(5).std()\n",
    "\n",
    "    # drop rows with NaNs resulting from indicators\n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b86ff1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def label_reversals(df, window=5, atr_period=14, atr_mult=1.5):\n",
    "    \"\"\"\n",
    "    Label reversal points using Swing High/Low + ATR filter.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Must contain ['High', 'Low', 'Close'] columns.\n",
    "    window : int\n",
    "        Number of candles on each side for swing detection.\n",
    "    atr_period : int\n",
    "        Period for ATR calculation.\n",
    "    atr_mult : float\n",
    "        Minimum price movement (in multiples of ATR) to qualify as a true reversal.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df : pd.DataFrame\n",
    "        Original dataframe with a new column 'Label':\n",
    "          0 = Top reversal\n",
    "          1 = No reversal\n",
    "          2 = Bottom reversal\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.copy().reset_index(drop=True)  # âœ… Ensure numeric index\n",
    "\n",
    "    # --- Calculate ATR (Average True Range) ---\n",
    "    high_low = df['high'] - df['low']\n",
    "    high_close = np.abs(df['high'] - df['close'].shift())\n",
    "    low_close = np.abs(df['low'] - df['close'].shift())\n",
    "    true_range = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)\n",
    "    df['ATR'] = true_range.rolling(atr_period).mean()\n",
    "\n",
    "    # --- Detect potential swing highs/lows ---\n",
    "    df['swing_high'] = df['high'][(df['high'] == df['high'].rolling(window*2+1, center=True).max())]\n",
    "    df['swing_low']  = df['low'][(df['low'] == df['low'].rolling(window*2+1, center=True).min())]\n",
    "\n",
    "    # --- Filter out minor reversals using ATR threshold ---\n",
    "    df['label'] = 1  # default: no reversal\n",
    "\n",
    "    for i in range(window, len(df) - window):\n",
    "        atr = df.loc[i, 'ATR']\n",
    "        if np.isnan(atr):\n",
    "            continue\n",
    "\n",
    "        # Check for swing high\n",
    "        if not np.isnan(df.loc[i, 'swing_high']):\n",
    "            # Price must have fallen more than atr_mult * ATR afterwards\n",
    "            future_min = df['low'].iloc[i:i+window].min()\n",
    "            if df.loc[i, 'high'] - future_min > atr_mult * atr:\n",
    "                df.loc[i, 'label'] = 0  # Top reversal\n",
    "\n",
    "        # Check for swing low\n",
    "        if not np.isnan(df.loc[i, 'swing_low']):\n",
    "            # Price must have risen more than atr_mult * ATR afterwards\n",
    "            future_max = df['high'].iloc[i:i+window].max()\n",
    "            if future_max - df.loc[i, 'low'] > atr_mult * atr:\n",
    "                df.loc[i, 'label'] = 2  # Bottom reversal\n",
    "\n",
    "    # --- Clean up temporary columns ---\n",
    "    df.drop(['swing_high', 'swing_low'], axis=1, inplace=True)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5b91255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 3) Create sequences (sliding windows)\n",
    "# ---------------------------\n",
    "def create_sequences_from_df(df, feature_names, window_size=10):\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "    arr = df[feature_names].values\n",
    "    labels = df['label'].values\n",
    "    for i in range(window_size, len(df)):\n",
    "        X_list.append(arr[i-window_size:i])\n",
    "        y_list.append(labels[i])\n",
    "    X = np.array(X_list, dtype=np.float32)\n",
    "    y = np.array(y_list, dtype=np.int64)\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1846c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 4) Model: CNN -> LSTM hybrid\n",
    "# ---------------------------\n",
    "class CNNLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, cnn_channels=64, kernel_size=3, lstm_hidden=128, lstm_layers=2, output_dim=3, dropout=0.3):\n",
    "        super().__init__()\n",
    "        # Conv1d expects (batch, channels, seq_len) -> we treat features as channels\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=input_dim, out_channels=cnn_channels, kernel_size=kernel_size, padding=kernel_size//2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(cnn_channels),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Conv1d(in_channels=cnn_channels, out_channels=cnn_channels, kernel_size=kernel_size, padding=kernel_size//2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(cnn_channels),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.lstm = nn.LSTM(input_size=cnn_channels, hidden_size=lstm_hidden, num_layers=lstm_layers, batch_first=True, dropout=dropout)\n",
    "        self.bn = nn.BatchNorm1d(lstm_hidden)\n",
    "        self.fc = nn.Linear(lstm_hidden, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, features)\n",
    "        x = x.permute(0, 2, 1)             # -> (batch, features, seq_len)\n",
    "        x = self.conv(x)                  # -> (batch, cnn_channels, seq_len)\n",
    "        x = x.permute(0, 2, 1)            # -> (batch, seq_len, cnn_channels)\n",
    "        out, _ = self.lstm(x)             # -> (batch, seq_len, lstm_hidden)\n",
    "        out = out[:, -1, :]               # last timestep\n",
    "        out = self.bn(out)                # batchnorm\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc2548a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 5) Training helper functions\n",
    "# ---------------------------\n",
    "def compute_class_weights(y):\n",
    "    classes, counts = np.unique(y, return_counts=True)\n",
    "    total = counts.sum()\n",
    "    num_classes = int(classes.max()) + 1\n",
    "    weights = np.ones(num_classes, dtype=np.float32)\n",
    "    for c, cnt in zip(classes, counts):\n",
    "        weights[c] = total / (cnt + 1e-8)\n",
    "    return torch.tensor(weights, dtype=torch.float32)\n",
    "\n",
    "def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for X_batch, y_batch in dataloader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device).squeeze().long()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * X_batch.size(0)\n",
    "    return running_loss / len(dataloader.dataset)\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    trues = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in dataloader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device).squeeze().long()\n",
    "            outputs = model(X_batch)\n",
    "            pred = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "            preds.append(pred)\n",
    "            trues.append(y_batch.cpu().numpy())\n",
    "    preds = np.concatenate(preds)\n",
    "    trues = np.concatenate(trues)\n",
    "    acc = accuracy_score(trues, preds)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(trues, preds, average='weighted', zero_division=0)\n",
    "    return acc, prec, rec, f1, trues, preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cbd153bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 6) Full pipeline function\n",
    "# ---------------------------\n",
    "def run_pipeline(df_raw,\n",
    "                 feature_names=None,\n",
    "                 window_size=10,\n",
    "                 test_size=0.2,\n",
    "                 batch_size=64,\n",
    "                 epochs=200,\n",
    "                 cnn_channels=64,\n",
    "                 lstm_hidden=128,\n",
    "                 lr=1e-3,\n",
    "                 weight_decay=1e-5,\n",
    "                 early_stopping_patience=8,\n",
    "                 random_state=42):\n",
    "    # 1) features\n",
    "    df = add_features(df_raw)\n",
    "    df = label_reversals(df, window=5, atr_period=14, atr_mult=1.5)\n",
    "    if feature_names is None:\n",
    "        # choose non-redundant features recommended earlier\n",
    "        feature_names = ['rsi','macd','signal','histogram','tsi','%k','%d','atr','obv','adx','cci','return_1','roll_mean_5','roll_std_5']\n",
    "    # build X,y sequences\n",
    "    X, y = create_sequences_from_df(df, feature_names, window_size=window_size)\n",
    "\n",
    "    # scale features (fit on training later) -> we will split first to avoid leakage\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, shuffle=False)  # time order preserved\n",
    "\n",
    "    # Flatten features for scaler, fit on training set only\n",
    "    nsamples, seq_len, nfeat = X_train.shape\n",
    "    scaler = StandardScaler()\n",
    "    X_train_flat = X_train.reshape(-1, nfeat)\n",
    "    X_train_flat = scaler.fit_transform(X_train_flat)\n",
    "    X_train = X_train_flat.reshape(nsamples, seq_len, nfeat)\n",
    "\n",
    "    # scale test\n",
    "    X_test_flat = X_test.reshape(-1, nfeat)\n",
    "    X_test_flat = scaler.transform(X_test_flat)\n",
    "    X_test = X_test_flat.reshape(X_test.shape[0], seq_len, nfeat)\n",
    "\n",
    "    joblib.dump(scaler, \"scaler_cnn_lstm.pkl\")\n",
    "    print(\"Saved scaler -> scaler_cnn_lstm.pkl\")\n",
    "\n",
    "    # Convert to tensors\n",
    "    X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_t = torch.tensor(y_train, dtype=torch.long).unsqueeze(1)\n",
    "    X_test_t = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test_t = torch.tensor(y_test, dtype=torch.long).unsqueeze(1)\n",
    "\n",
    "    # compute class weights and sampler for balanced batch sampling\n",
    "    class_weights = compute_class_weights(y_train).to(device)\n",
    "    print(\"Class weights:\", class_weights.cpu().numpy())\n",
    "\n",
    "    # WeightedRandomSampler to mitigate imbalance while training\n",
    "    # weight for each sample = 1 / count(label)\n",
    "    labels_np = y_train\n",
    "    class_sample_count = np.array([len(np.where(labels_np == t)[0]) for t in np.unique(labels_np)])\n",
    "    weight = 1. / class_sample_count\n",
    "    samples_weight = np.array([weight[t] for t in labels_np])\n",
    "    samples_weight = torch.from_numpy(samples_weight).double()\n",
    "    sampler = WeightedRandomSampler(samples_weight, num_samples=len(samples_weight), replacement=True)\n",
    "\n",
    "    train_dataset = TensorDataset(X_train_t, y_train_t)\n",
    "    test_dataset = TensorDataset(X_test_t, y_test_t)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler, drop_last=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # model init\n",
    "    model = CNNLSTM(input_dim=nfeat, cnn_channels=cnn_channels, lstm_hidden=lstm_hidden, lstm_layers=2, output_dim=int(y.max())+1).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "    # scheduler & early stopping\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.7, patience=4)\n",
    "    best_val = -np.inf\n",
    "    best_epoch = -1\n",
    "    patience = early_stopping_patience\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        start = time.time()\n",
    "        train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_acc, val_prec, val_rec, val_f1, _, _ = evaluate(model, test_loader, device)\n",
    "        scheduler.step(val_acc)  # use val acc as monitor\n",
    "\n",
    "        elapsed = time.time() - start\n",
    "        print(f\"Epoch {epoch}/{epochs} | train_loss={train_loss:.5f} val_acc={val_acc:.4f} val_f1={val_f1:.4f} time={elapsed:.1f}s\")\n",
    "\n",
    "        \"\"\"# early stopping on val acc\n",
    "        if val_acc > best_val + 1e-5:\n",
    "            best_val = val_acc\n",
    "            best_epoch = epoch\n",
    "            torch.save(model.state_dict(), \"best_cnn_lstm.pth\")\n",
    "            print(\"  Saved best model at epoch\", epoch)\n",
    "            patience = early_stopping_patience  # reset patience\n",
    "        else:\n",
    "            patience -= 1\n",
    "            if patience <= 0:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\"\"\"\n",
    "\n",
    "    # load best model and final evaluation\n",
    "    model.load_state_dict(torch.load(\"best_cnn_lstm.pth\", map_location=device))\n",
    "    final_acc, final_prec, final_rec, final_f1, trues, preds = evaluate(model, test_loader, device)\n",
    "    print(\"Final Test -- acc: %.4f prec: %.4f rec: %.4f f1: %.4f\" % (final_acc, final_prec, final_rec, final_f1))\n",
    "\n",
    "    # Save final artifacts\n",
    "    torch.save(model.state_dict(), \"cnn_lstm_final.pth\")\n",
    "    print(\"Saved final model -> cnn_lstm_final.pth\")\n",
    "\n",
    "    return {\n",
    "        'model': model,\n",
    "        'scaler': scaler,\n",
    "        'feature_names': feature_names,\n",
    "        'metrics': (final_acc, final_prec, final_rec, final_f1),\n",
    "        'y_true': trues,\n",
    "        'y_pred': preds\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8506a930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load your OHLCV csv into df_raw\n",
    "import ccxt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Initialize Binance\n",
    "exchange = ccxt.binance()\n",
    "symbol = 'BTC/USDT'\n",
    "timeframe = '15m'  # hourly candles\n",
    "limit = 10000  # number of candles to fetch\n",
    "\n",
    "# Fetch OHLCV data\n",
    "ohlcv = exchange.fetch_ohlcv(symbol, timeframe=timeframe, limit=limit)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(ohlcv, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')\n",
    "df.set_index('timestamp', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "585f8133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved scaler -> scaler_cnn_lstm.pkl\n",
      "Class weights: [19.897436   1.1165468 18.47619  ]\n",
      "Epoch 1/200 | train_loss=0.97363 val_acc=0.0564 val_f1=0.0089 time=0.9s\n",
      "Epoch 2/200 | train_loss=0.76122 val_acc=0.0462 val_f1=0.0060 time=1.1s\n",
      "Epoch 3/200 | train_loss=0.53735 val_acc=0.0513 val_f1=0.0074 time=1.0s\n",
      "Epoch 4/200 | train_loss=0.51110 val_acc=0.0821 val_f1=0.0587 time=0.8s\n",
      "Epoch 5/200 | train_loss=0.44447 val_acc=0.0513 val_f1=0.0075 time=0.9s\n",
      "Epoch 6/200 | train_loss=0.39505 val_acc=0.0513 val_f1=0.0076 time=0.9s\n",
      "Epoch 7/200 | train_loss=0.32863 val_acc=0.0513 val_f1=0.0074 time=0.9s\n",
      "Epoch 8/200 | train_loss=0.33831 val_acc=0.0513 val_f1=0.0074 time=0.9s\n",
      "Epoch 9/200 | train_loss=0.27995 val_acc=0.0564 val_f1=0.0086 time=0.8s\n",
      "Epoch 10/200 | train_loss=0.25859 val_acc=0.0564 val_f1=0.0087 time=0.7s\n",
      "Epoch 11/200 | train_loss=0.23816 val_acc=0.0718 val_f1=0.0389 time=0.7s\n",
      "Epoch 12/200 | train_loss=0.25633 val_acc=0.0513 val_f1=0.0074 time=0.6s\n",
      "Epoch 13/200 | train_loss=0.30350 val_acc=0.0564 val_f1=0.0085 time=0.7s\n",
      "Epoch 14/200 | train_loss=0.26194 val_acc=0.1077 val_f1=0.1061 time=0.6s\n",
      "Epoch 15/200 | train_loss=0.23782 val_acc=0.1846 val_f1=0.2323 time=0.7s\n",
      "Epoch 16/200 | train_loss=0.19659 val_acc=0.1487 val_f1=0.1753 time=0.7s\n",
      "Epoch 17/200 | train_loss=0.18399 val_acc=0.2821 val_f1=0.3977 time=0.8s\n",
      "Epoch 18/200 | train_loss=0.20216 val_acc=0.3231 val_f1=0.4382 time=0.6s\n",
      "Epoch 19/200 | train_loss=0.20563 val_acc=0.3385 val_f1=0.4580 time=0.6s\n",
      "Epoch 20/200 | train_loss=0.18840 val_acc=0.3436 val_f1=0.4677 time=0.7s\n",
      "Epoch 21/200 | train_loss=0.17351 val_acc=0.3487 val_f1=0.4690 time=0.7s\n",
      "Epoch 22/200 | train_loss=0.20575 val_acc=0.3538 val_f1=0.4743 time=0.6s\n",
      "Epoch 23/200 | train_loss=0.19564 val_acc=0.4000 val_f1=0.5229 time=0.6s\n",
      "Epoch 24/200 | train_loss=0.20552 val_acc=0.3692 val_f1=0.4904 time=0.6s\n",
      "Epoch 25/200 | train_loss=0.19182 val_acc=0.4051 val_f1=0.5280 time=0.6s\n",
      "Epoch 26/200 | train_loss=0.16851 val_acc=0.4205 val_f1=0.5410 time=0.6s\n",
      "Epoch 27/200 | train_loss=0.15780 val_acc=0.3179 val_f1=0.4354 time=0.6s\n",
      "Epoch 28/200 | train_loss=0.17939 val_acc=0.2256 val_f1=0.3094 time=0.7s\n",
      "Epoch 29/200 | train_loss=0.17016 val_acc=0.3179 val_f1=0.4325 time=0.8s\n",
      "Epoch 30/200 | train_loss=0.14784 val_acc=0.4051 val_f1=0.5261 time=0.6s\n",
      "Epoch 31/200 | train_loss=0.14165 val_acc=0.5077 val_f1=0.6191 time=0.7s\n",
      "Epoch 32/200 | train_loss=0.13354 val_acc=0.4923 val_f1=0.6059 time=0.6s\n",
      "Epoch 33/200 | train_loss=0.12555 val_acc=0.3590 val_f1=0.4795 time=0.6s\n",
      "Epoch 34/200 | train_loss=0.16371 val_acc=0.4154 val_f1=0.5359 time=0.6s\n",
      "Epoch 35/200 | train_loss=0.14446 val_acc=0.4410 val_f1=0.5615 time=0.7s\n",
      "Epoch 36/200 | train_loss=0.17457 val_acc=0.5179 val_f1=0.6284 time=0.6s\n",
      "Epoch 37/200 | train_loss=0.14845 val_acc=0.4923 val_f1=0.6069 time=0.6s\n",
      "Epoch 38/200 | train_loss=0.12981 val_acc=0.4103 val_f1=0.5309 time=0.6s\n",
      "Epoch 39/200 | train_loss=0.12644 val_acc=0.4564 val_f1=0.5742 time=0.6s\n",
      "Epoch 40/200 | train_loss=0.13110 val_acc=0.4256 val_f1=0.5456 time=0.7s\n",
      "Epoch 41/200 | train_loss=0.15070 val_acc=0.4256 val_f1=0.5459 time=0.6s\n",
      "Epoch 42/200 | train_loss=0.15123 val_acc=0.4769 val_f1=0.5941 time=0.6s\n",
      "Epoch 43/200 | train_loss=0.15466 val_acc=0.4769 val_f1=0.5904 time=0.8s\n",
      "Epoch 44/200 | train_loss=0.14163 val_acc=0.4000 val_f1=0.5210 time=0.7s\n",
      "Epoch 45/200 | train_loss=0.14611 val_acc=0.3077 val_f1=0.4256 time=0.7s\n",
      "Epoch 46/200 | train_loss=0.13620 val_acc=0.3795 val_f1=0.5006 time=0.6s\n",
      "Epoch 47/200 | train_loss=0.12351 val_acc=0.4923 val_f1=0.6038 time=0.6s\n",
      "Epoch 48/200 | train_loss=0.12584 val_acc=0.4410 val_f1=0.5580 time=0.7s\n",
      "Epoch 49/200 | train_loss=0.11579 val_acc=0.3897 val_f1=0.5108 time=0.6s\n",
      "Epoch 50/200 | train_loss=0.10744 val_acc=0.3795 val_f1=0.5006 time=0.7s\n",
      "Epoch 51/200 | train_loss=0.10044 val_acc=0.4051 val_f1=0.5260 time=0.6s\n",
      "Epoch 52/200 | train_loss=0.11887 val_acc=0.4667 val_f1=0.5814 time=0.6s\n",
      "Epoch 53/200 | train_loss=0.11925 val_acc=0.4359 val_f1=0.5544 time=0.6s\n",
      "Epoch 54/200 | train_loss=0.13314 val_acc=0.4513 val_f1=0.5683 time=0.6s\n",
      "Epoch 55/200 | train_loss=0.11673 val_acc=0.4513 val_f1=0.5684 time=0.6s\n",
      "Epoch 56/200 | train_loss=0.13436 val_acc=0.5128 val_f1=0.6212 time=0.7s\n",
      "Epoch 57/200 | train_loss=0.13714 val_acc=0.5026 val_f1=0.6123 time=0.6s\n",
      "Epoch 58/200 | train_loss=0.11792 val_acc=0.5590 val_f1=0.6580 time=0.6s\n",
      "Epoch 59/200 | train_loss=0.10471 val_acc=0.5385 val_f1=0.6420 time=0.6s\n",
      "Epoch 60/200 | train_loss=0.10905 val_acc=0.4821 val_f1=0.5950 time=0.6s\n",
      "Epoch 61/200 | train_loss=0.10523 val_acc=0.4821 val_f1=0.5952 time=0.6s\n",
      "Epoch 62/200 | train_loss=0.10359 val_acc=0.4718 val_f1=0.5866 time=0.6s\n",
      "Epoch 63/200 | train_loss=0.11103 val_acc=0.5128 val_f1=0.6213 time=0.6s\n",
      "Epoch 64/200 | train_loss=0.09765 val_acc=0.5128 val_f1=0.6213 time=0.7s\n",
      "Epoch 65/200 | train_loss=0.10947 val_acc=0.5077 val_f1=0.6169 time=0.6s\n",
      "Epoch 66/200 | train_loss=0.09733 val_acc=0.4359 val_f1=0.5546 time=0.6s\n",
      "Epoch 67/200 | train_loss=0.10488 val_acc=0.4308 val_f1=0.5499 time=0.6s\n",
      "Epoch 68/200 | train_loss=0.12110 val_acc=0.4718 val_f1=0.5864 time=0.8s\n",
      "Epoch 69/200 | train_loss=0.11890 val_acc=0.4615 val_f1=0.5772 time=0.6s\n",
      "Epoch 70/200 | train_loss=0.10253 val_acc=0.4872 val_f1=0.5999 time=0.6s\n",
      "Epoch 71/200 | train_loss=0.10481 val_acc=0.4872 val_f1=0.5999 time=0.6s\n",
      "Epoch 72/200 | train_loss=0.09880 val_acc=0.4769 val_f1=0.5910 time=0.6s\n",
      "Epoch 73/200 | train_loss=0.10133 val_acc=0.4718 val_f1=0.5867 time=0.6s\n",
      "Epoch 74/200 | train_loss=0.09808 val_acc=0.4923 val_f1=0.6043 time=0.6s\n",
      "Epoch 75/200 | train_loss=0.10127 val_acc=0.5487 val_f1=0.6503 time=0.6s\n",
      "Epoch 76/200 | train_loss=0.10790 val_acc=0.4974 val_f1=0.6087 time=1.0s\n",
      "Epoch 77/200 | train_loss=0.10388 val_acc=0.5231 val_f1=0.6300 time=0.7s\n",
      "Epoch 78/200 | train_loss=0.09273 val_acc=0.4974 val_f1=0.6089 time=0.7s\n",
      "Epoch 79/200 | train_loss=0.11710 val_acc=0.4821 val_f1=0.5958 time=0.6s\n",
      "Epoch 80/200 | train_loss=0.09970 val_acc=0.4564 val_f1=0.5732 time=0.6s\n",
      "Epoch 81/200 | train_loss=0.11052 val_acc=0.5077 val_f1=0.6173 time=0.7s\n",
      "Epoch 82/200 | train_loss=0.11038 val_acc=0.5282 val_f1=0.6341 time=0.8s\n",
      "Epoch 83/200 | train_loss=0.10213 val_acc=0.5077 val_f1=0.6173 time=0.6s\n",
      "Epoch 84/200 | train_loss=0.09391 val_acc=0.4667 val_f1=0.5824 time=0.6s\n",
      "Epoch 85/200 | train_loss=0.11186 val_acc=0.4513 val_f1=0.5685 time=0.6s\n",
      "Epoch 86/200 | train_loss=0.09693 val_acc=0.4359 val_f1=0.5546 time=0.7s\n",
      "Epoch 87/200 | train_loss=0.10733 val_acc=0.4513 val_f1=0.5686 time=0.7s\n",
      "Epoch 88/200 | train_loss=0.10588 val_acc=0.5179 val_f1=0.6256 time=0.7s\n",
      "Epoch 89/200 | train_loss=0.10620 val_acc=0.4462 val_f1=0.5642 time=0.9s\n",
      "Epoch 90/200 | train_loss=0.09365 val_acc=0.4564 val_f1=0.5731 time=0.8s\n",
      "Epoch 91/200 | train_loss=0.10867 val_acc=0.4923 val_f1=0.6043 time=0.7s\n",
      "Epoch 92/200 | train_loss=0.10635 val_acc=0.4769 val_f1=0.5911 time=0.7s\n",
      "Epoch 93/200 | train_loss=0.11899 val_acc=0.4615 val_f1=0.5775 time=0.7s\n",
      "Epoch 94/200 | train_loss=0.11696 val_acc=0.5333 val_f1=0.6383 time=0.7s\n",
      "Epoch 95/200 | train_loss=0.09685 val_acc=0.5026 val_f1=0.6129 time=0.6s\n",
      "Epoch 96/200 | train_loss=0.10591 val_acc=0.5282 val_f1=0.6339 time=0.6s\n",
      "Epoch 97/200 | train_loss=0.10965 val_acc=0.4205 val_f1=0.5404 time=0.6s\n",
      "Epoch 98/200 | train_loss=0.10511 val_acc=0.5179 val_f1=0.6256 time=0.6s\n",
      "Epoch 99/200 | train_loss=0.10286 val_acc=0.4564 val_f1=0.5731 time=0.7s\n",
      "Epoch 100/200 | train_loss=0.09259 val_acc=0.4872 val_f1=0.5999 time=0.6s\n",
      "Epoch 101/200 | train_loss=0.10119 val_acc=0.3897 val_f1=0.5092 time=0.6s\n",
      "Epoch 102/200 | train_loss=0.09365 val_acc=0.4513 val_f1=0.5685 time=0.6s\n",
      "Epoch 103/200 | train_loss=0.09416 val_acc=0.4667 val_f1=0.5822 time=0.6s\n",
      "Epoch 104/200 | train_loss=0.10083 val_acc=0.4974 val_f1=0.6083 time=0.6s\n",
      "Epoch 105/200 | train_loss=0.10682 val_acc=0.4051 val_f1=0.5255 time=0.6s\n",
      "Epoch 106/200 | train_loss=0.10932 val_acc=0.4821 val_f1=0.5956 time=0.6s\n",
      "Epoch 107/200 | train_loss=0.09726 val_acc=0.4667 val_f1=0.5823 time=0.7s\n",
      "Epoch 108/200 | train_loss=0.10130 val_acc=0.5333 val_f1=0.6380 time=0.7s\n",
      "Epoch 109/200 | train_loss=0.11618 val_acc=0.4718 val_f1=0.5868 time=0.6s\n",
      "Epoch 110/200 | train_loss=0.10404 val_acc=0.5179 val_f1=0.6257 time=0.6s\n",
      "Epoch 111/200 | train_loss=0.09669 val_acc=0.4462 val_f1=0.5639 time=0.6s\n",
      "Epoch 112/200 | train_loss=0.10752 val_acc=0.5436 val_f1=0.6461 time=0.6s\n",
      "Epoch 113/200 | train_loss=0.11054 val_acc=0.4564 val_f1=0.5732 time=0.7s\n",
      "Epoch 114/200 | train_loss=0.12467 val_acc=0.5282 val_f1=0.6337 time=0.6s\n",
      "Epoch 115/200 | train_loss=0.09789 val_acc=0.5128 val_f1=0.6214 time=0.7s\n",
      "Epoch 116/200 | train_loss=0.09793 val_acc=0.5179 val_f1=0.6254 time=0.7s\n",
      "Epoch 117/200 | train_loss=0.11950 val_acc=0.5077 val_f1=0.6169 time=0.8s\n",
      "Epoch 118/200 | train_loss=0.10647 val_acc=0.4769 val_f1=0.5910 time=0.7s\n",
      "Epoch 119/200 | train_loss=0.10322 val_acc=0.4821 val_f1=0.5955 time=0.8s\n",
      "Epoch 120/200 | train_loss=0.10224 val_acc=0.5385 val_f1=0.6423 time=0.7s\n",
      "Epoch 121/200 | train_loss=0.11112 val_acc=0.5077 val_f1=0.6170 time=0.7s\n",
      "Epoch 122/200 | train_loss=0.08437 val_acc=0.4974 val_f1=0.6084 time=0.8s\n",
      "Epoch 123/200 | train_loss=0.10112 val_acc=0.4821 val_f1=0.5955 time=0.7s\n",
      "Epoch 124/200 | train_loss=0.11663 val_acc=0.5128 val_f1=0.6216 time=0.7s\n",
      "Epoch 125/200 | train_loss=0.10240 val_acc=0.4564 val_f1=0.5732 time=1.1s\n",
      "Epoch 126/200 | train_loss=0.10909 val_acc=0.4256 val_f1=0.5449 time=0.7s\n",
      "Epoch 127/200 | train_loss=0.11868 val_acc=0.5077 val_f1=0.6168 time=0.6s\n",
      "Epoch 128/200 | train_loss=0.09185 val_acc=0.4615 val_f1=0.5776 time=0.7s\n",
      "Epoch 129/200 | train_loss=0.11366 val_acc=0.4821 val_f1=0.5955 time=0.7s\n",
      "Epoch 130/200 | train_loss=0.10765 val_acc=0.4205 val_f1=0.5405 time=0.7s\n",
      "Epoch 131/200 | train_loss=0.09724 val_acc=0.4667 val_f1=0.5824 time=0.8s\n",
      "Epoch 132/200 | train_loss=0.10295 val_acc=0.4718 val_f1=0.5866 time=0.7s\n",
      "Epoch 133/200 | train_loss=0.09279 val_acc=0.5282 val_f1=0.6341 time=0.8s\n",
      "Epoch 134/200 | train_loss=0.10220 val_acc=0.5077 val_f1=0.6173 time=0.7s\n",
      "Epoch 135/200 | train_loss=0.10815 val_acc=0.4359 val_f1=0.5547 time=0.8s\n",
      "Epoch 136/200 | train_loss=0.11096 val_acc=0.5077 val_f1=0.6171 time=0.7s\n",
      "Epoch 137/200 | train_loss=0.09988 val_acc=0.5026 val_f1=0.6128 time=0.7s\n",
      "Epoch 138/200 | train_loss=0.11339 val_acc=0.4564 val_f1=0.5732 time=0.7s\n",
      "Epoch 139/200 | train_loss=0.10835 val_acc=0.4923 val_f1=0.6046 time=0.6s\n",
      "Epoch 140/200 | train_loss=0.09481 val_acc=0.4872 val_f1=0.5999 time=0.7s\n",
      "Epoch 141/200 | train_loss=0.11819 val_acc=0.5128 val_f1=0.6214 time=0.6s\n",
      "Epoch 142/200 | train_loss=0.10469 val_acc=0.4667 val_f1=0.5820 time=0.6s\n",
      "Epoch 143/200 | train_loss=0.11076 val_acc=0.5026 val_f1=0.6129 time=0.7s\n",
      "Epoch 144/200 | train_loss=0.11627 val_acc=0.4872 val_f1=0.5999 time=0.7s\n",
      "Epoch 145/200 | train_loss=0.09729 val_acc=0.4667 val_f1=0.5824 time=0.7s\n",
      "Epoch 146/200 | train_loss=0.09384 val_acc=0.4667 val_f1=0.5824 time=0.6s\n",
      "Epoch 147/200 | train_loss=0.10290 val_acc=0.5590 val_f1=0.6580 time=0.8s\n",
      "Epoch 148/200 | train_loss=0.09883 val_acc=0.3744 val_f1=0.4955 time=0.8s\n",
      "Epoch 149/200 | train_loss=0.09916 val_acc=0.5026 val_f1=0.6128 time=0.8s\n",
      "Epoch 150/200 | train_loss=0.10784 val_acc=0.4564 val_f1=0.5731 time=0.7s\n",
      "Epoch 151/200 | train_loss=0.11669 val_acc=0.4667 val_f1=0.5824 time=0.7s\n",
      "Epoch 152/200 | train_loss=0.11444 val_acc=0.4205 val_f1=0.5404 time=0.6s\n",
      "Epoch 153/200 | train_loss=0.07922 val_acc=0.4308 val_f1=0.5497 time=0.7s\n",
      "Epoch 154/200 | train_loss=0.09413 val_acc=0.4667 val_f1=0.5822 time=0.7s\n",
      "Epoch 155/200 | train_loss=0.08946 val_acc=0.4923 val_f1=0.6042 time=0.6s\n",
      "Epoch 156/200 | train_loss=0.10158 val_acc=0.4410 val_f1=0.5591 time=0.7s\n",
      "Epoch 157/200 | train_loss=0.09568 val_acc=0.4154 val_f1=0.5352 time=1.2s\n",
      "Epoch 158/200 | train_loss=0.09966 val_acc=0.4256 val_f1=0.5450 time=0.7s\n",
      "Epoch 159/200 | train_loss=0.09094 val_acc=0.5179 val_f1=0.6256 time=0.7s\n",
      "Epoch 160/200 | train_loss=0.09243 val_acc=0.5026 val_f1=0.6127 time=0.7s\n",
      "Epoch 161/200 | train_loss=0.10152 val_acc=0.5282 val_f1=0.6340 time=0.7s\n",
      "Epoch 162/200 | train_loss=0.09269 val_acc=0.4821 val_f1=0.5957 time=0.7s\n",
      "Epoch 163/200 | train_loss=0.10105 val_acc=0.5436 val_f1=0.6462 time=0.8s\n",
      "Epoch 164/200 | train_loss=0.11287 val_acc=0.5385 val_f1=0.6421 time=1.0s\n",
      "Epoch 165/200 | train_loss=0.09355 val_acc=0.5026 val_f1=0.6128 time=0.7s\n",
      "Epoch 166/200 | train_loss=0.09137 val_acc=0.5026 val_f1=0.6129 time=1.3s\n",
      "Epoch 167/200 | train_loss=0.10323 val_acc=0.4000 val_f1=0.5205 time=1.5s\n",
      "Epoch 168/200 | train_loss=0.10737 val_acc=0.5231 val_f1=0.6299 time=0.6s\n",
      "Epoch 169/200 | train_loss=0.10547 val_acc=0.5077 val_f1=0.6173 time=0.6s\n",
      "Epoch 170/200 | train_loss=0.11155 val_acc=0.5385 val_f1=0.6422 time=0.6s\n",
      "Epoch 171/200 | train_loss=0.10268 val_acc=0.5385 val_f1=0.6421 time=0.6s\n",
      "Epoch 172/200 | train_loss=0.11125 val_acc=0.5128 val_f1=0.6214 time=0.6s\n",
      "Epoch 173/200 | train_loss=0.11460 val_acc=0.5179 val_f1=0.6255 time=0.6s\n",
      "Epoch 174/200 | train_loss=0.11131 val_acc=0.4872 val_f1=0.6001 time=0.6s\n",
      "Epoch 175/200 | train_loss=0.10763 val_acc=0.4821 val_f1=0.5955 time=0.6s\n",
      "Epoch 176/200 | train_loss=0.10585 val_acc=0.4564 val_f1=0.5728 time=0.7s\n",
      "Epoch 177/200 | train_loss=0.10459 val_acc=0.5026 val_f1=0.6129 time=0.9s\n",
      "Epoch 178/200 | train_loss=0.11375 val_acc=0.4872 val_f1=0.5997 time=0.9s\n",
      "Epoch 179/200 | train_loss=0.10272 val_acc=0.5128 val_f1=0.6213 time=0.8s\n",
      "Epoch 180/200 | train_loss=0.10288 val_acc=0.5026 val_f1=0.6128 time=0.6s\n",
      "Epoch 181/200 | train_loss=0.10111 val_acc=0.4564 val_f1=0.5733 time=0.6s\n",
      "Epoch 182/200 | train_loss=0.10268 val_acc=0.5077 val_f1=0.6171 time=0.7s\n",
      "Epoch 183/200 | train_loss=0.09297 val_acc=0.5282 val_f1=0.6338 time=0.7s\n",
      "Epoch 184/200 | train_loss=0.11284 val_acc=0.4923 val_f1=0.6042 time=0.7s\n",
      "Epoch 185/200 | train_loss=0.10304 val_acc=0.4974 val_f1=0.6083 time=0.6s\n",
      "Epoch 186/200 | train_loss=0.10136 val_acc=0.5590 val_f1=0.6579 time=1.0s\n",
      "Epoch 187/200 | train_loss=0.11248 val_acc=0.5692 val_f1=0.6658 time=0.7s\n",
      "Epoch 188/200 | train_loss=0.09786 val_acc=0.5590 val_f1=0.6580 time=0.8s\n",
      "Epoch 189/200 | train_loss=0.10623 val_acc=0.5231 val_f1=0.6295 time=0.9s\n",
      "Epoch 190/200 | train_loss=0.10175 val_acc=0.5385 val_f1=0.6422 time=0.7s\n",
      "Epoch 191/200 | train_loss=0.10023 val_acc=0.4615 val_f1=0.5779 time=1.0s\n",
      "Epoch 192/200 | train_loss=0.10269 val_acc=0.5231 val_f1=0.6299 time=0.9s\n",
      "Epoch 193/200 | train_loss=0.10283 val_acc=0.4821 val_f1=0.5953 time=1.2s\n",
      "Epoch 194/200 | train_loss=0.09299 val_acc=0.4821 val_f1=0.5954 time=0.6s\n",
      "Epoch 195/200 | train_loss=0.11947 val_acc=0.5282 val_f1=0.6337 time=0.6s\n",
      "Epoch 196/200 | train_loss=0.09013 val_acc=0.4923 val_f1=0.6046 time=0.9s\n",
      "Epoch 197/200 | train_loss=0.10235 val_acc=0.5231 val_f1=0.6299 time=0.9s\n",
      "Epoch 198/200 | train_loss=0.10334 val_acc=0.3744 val_f1=0.4958 time=0.6s\n",
      "Epoch 199/200 | train_loss=0.09347 val_acc=0.4974 val_f1=0.6087 time=0.6s\n",
      "Epoch 200/200 | train_loss=0.09484 val_acc=0.4667 val_f1=0.5822 time=0.8s\n",
      "Final Test -- acc: 0.0974 prec: 0.9183 rec: 0.0974 f1: 0.0881\n",
      "Saved final model -> cnn_lstm_final.pth\n",
      "Pipeline complete. Metrics: (0.09743589743589744, 0.9182625067240453, 0.09743589743589744, 0.08809424615755307)\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# 7) Usage example\n",
    "# ---------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "\n",
    "    greg = run_pipeline(df)\n",
    "    print(\"Pipeline complete. Metrics:\", greg['metrics'])\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
